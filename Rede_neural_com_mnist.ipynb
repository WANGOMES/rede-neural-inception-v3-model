{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlkqZCQAZKzPnvTi21fDfk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WANGOMES/rede-neural-inception-v3-model/blob/main/Rede_neural_com_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nALq5c4ktWqf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor() #definindo a conversão da imagem para tensor.\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform) #Carrega treino do dataset.\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) #Cria um buffer para pegar os dados por partes.\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform) #Carrega parte da validação do dataset.\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) #Cria um buffer para pegar os dados por partes."
      ],
      "metadata": {
        "id": "4ldiG3Cougu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = dataiter.next()\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ttwO5JcNyMJe",
        "outputId": "a219cfe4-5211-45df-a5dd-e66a77834f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(imagens[0].shape) #para verificar as dimensões do tensor de cada imagem.\n",
        "print(etiquetas[0].shape) #para verificar as dimensões do tensor de cada etiqueta."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QVbJAnVy9IQ",
        "outputId": "7222249e-676b-450d-eb22-6274f898658e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Modelo(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Modelo, self).__init__()\n",
        "    self.linear1 = nn.Linear(28*28, 128) #camada de entrada, 784 neurônios que se ligam a 128\n",
        "    self.linear2 = nn.Linear(128, 64) #camada interna 1, 128 neurônios que se ligam a 64\n",
        "    self.linear3 = nn.Linear(64, 10) #camada interna 2, 64 neurônios que se ligam a 10\n",
        "    # para a camada de saída não é necessário definir nada pos só precisamos pegar o output da camada interna 2\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = F.relu(self.linear1(X)) #função de ativação da camada de entrada para a camada interna 1\n",
        "    X = F.relu(self.linear2(X)) #função de ativação da camada interna 1 para a camada interna 2\n",
        "    X = self.linear3(X) #função de ativação da camada interna 2 para a camada de saída, nesse caso f(x) = x\n",
        "    return F.log_softmax(X, dim=1) #dados utilizados para calcular a perda"
      ],
      "metadata": {
        "id": "yWQVgGJs1VPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treino(modelo, trainloader, device):\n",
        "\n",
        "  otimizador = optim.SGD(modelo.paramters(), lr=0.01, momentum=0.5) # deine a política de atualização dos pesps e da bias\n",
        "  inicio = time() # timer para sabermos quanto tempo levou o treino\n",
        "\n",
        "  criterio = nn.NLLLoss() # definindo o critério para calcular a perda\n",
        "  EPOCHS = 10 # números de epochs que o algoritmo rodará\n",
        "  modelo.train() #ativando o mode de treinamento do modelo\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada = 0 # inicialização da perda acumulada da epoch em questão\n",
        "\n",
        "    for imagens, etiquetas in trainloader:\n",
        "\n",
        "      imagens = imagens.view(imagens.shape[0], -1) # convertendo as imagens para 'vetores' de 28*28 casas\n",
        "      otimizador.zero_grad() # zerando os gradientes por conta do ciclo anterior\n",
        "\n",
        "      output = modelo(imagens.to(device)) # colocando os dados no modelo\n",
        "      perda_instantanea = criterio(output, etiquetas.to(device)) # calculando a perda da epoch em questão\n",
        "\n",
        "      perda_instantanea.backward() # back progation a partir da perda\n",
        "\n",
        "      otimizador.step() #atualizando os pesos e a bias\n",
        "\n",
        "      perda_acumulada += perda_instantanea.item() #atualização da perda acumulada\n",
        "\n",
        "\n",
        "    else:\n",
        "      print(\"Epoch {} - Perda resultante: {}\".format(epoch + 1, perda_acumulada/len(trainloader)))\n",
        "  print(\"\\nTempo de treino (em minutos) = \", (time() - inicio)/60)"
      ],
      "metadata": {
        "id": "qF_2TmYi3ry4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo, valloader, device):\n",
        "  conta_corretas, conta_todas = 0, 0\n",
        "  for imagens, etiquetas in valloader:\n",
        "    for i in range(len(etiquetas)):\n",
        "      img = imagens[i].view(1, 784)\n",
        "      # desativar o autograd para acelerar a validacao. Grafos computacionais dinamicos tem um custo alto de processamento\n",
        "      with torch.no_grad():\n",
        "        logps = modelo(img.to(device)) # output do modelo em escala logaritmica\n",
        "\n",
        "        ps = torch.exp(logps) # convert output para escala normal (lembrando que é um tensor)\n",
        "        probab = list(ps.cpu().numpy()[0])\n",
        "        etiqueta_pred = probab.index(max(probab)) # converte o tensor em um número, no caso, o número que o modelo previu como correto\n",
        "        etiqueta_certa = etiquetas.numpy()[i]\n",
        "        if(etiqueta_certa == etiqueta_pred): # compara a previsao com o valor correto\n",
        "          conta_corretas += 1\n",
        "        conta_todas += 1\n",
        "  print(\"Total de imagens testadas = \", conta_todas)\n",
        "  print(\"\\nPrecisão do modelo = {}%\".format(conta_corretas*100/conta_todas))"
      ],
      "metadata": {
        "id": "ykCCsLn27trw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShuKG79d_5V2",
        "outputId": "aa87a240-7e99-46b6-a2d7-d30194b0c276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ]
}